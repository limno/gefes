Sample "{{sample_short_name}}"
==============================

General Information
-------------------
This sample has the code `{{sample_short_name}}` and is named "{{sample_long_name}}". It is part of the project code `{{project_short_name}}` called "{{project_long_name}}" along with {{project_other_samples}} other samples.

-------

Meta-data details
-----------------
The meta data associated with this sample can be found in the following JSON file:

> <{{json_url}}>

Here is a summary of all the information pertaining to this sample:

    {{information}}

-------

Processing
----------
This report (and all the analysis) was generated using the GEFES project at:

> <{{project_url}}>

Version `{{project_version}}` of the pipeline was used. The exact git hash of the latest commit was:

    {{git_hash}}

also more simply referred to by its shortened tag `{{git_tag}}`.

This document was generated at `{{now}}`.

A brief overview of what happens to the data can be viewed online here:

> <https://github.com/limno/gefes/blob/{{git_branch}}/documentation/flowchart.pdf?raw=true>

The results and all the files generated for this sample can be found at:

    {{results_directory}}

-------

Raw data
--------
* The forward read file weighed *{{fwd_size}}* and contained *{{fwd_count}}* reads.
* The reverse read file weighed *{{rev_size}}* and contained *{{rev_count}}* reads.

More information about the raw output of the sequencer for this sample can be found in the HTML report generated by the Illumina software here:

    {{illumina_report}}

The average quality per base can be seen in figure \ref{raw_per_base_qual} and the average quality per sequence in figure \ref{raw_per_seq_qual}.

{{raw_per_base_qual}}

{{raw_per_seq_qual}}

-------

Preprocessing
-------------
Next, we filter the sequences based on the following rules:

* We remove any bases from the extremities of each sequence if they are under the quality threshold of {{phred_threshold}}.

* We run a {{window_size}} base pair window over the sequences and check that the quality doesn't drop below the same threshold of {{phred_threshold}}. If it does, we trim the read and keep the longest stretch.

* If the resulting trimmed read is shorter than {{length_threshold}} base pairs or contains any undetermined "N" bases, we keep discard it.

* In the event that one of the reads in a pair gets discarded while the other one does not, we place the resulting singleton in a separate "singles" FASTQ file.

This leaves us with {{remaining_percent}} of the original sequences organized in {{remaining_pairs}} pairs and {{remaining_singles}} singletons.

Of course, now, not all sequences have the same length and we have created a distribution of read sizes as seen in figure \ref{cleaned_len_dist}.

{{cleaned_len_dist}}

The singletons have their own length distribution shown in figure \ref{singletons_len_dist}:

{{singletons_len_dist}}

We can look again at the per base quality and the per sequence quality of the pairs after cleaning in figure \ref{cleaned_per_base_qual} and \ref{cleaned_per_seq_qual}:

{{cleaned_per_base_qual}}

{{cleaned_per_seq_qual}}

-------

Rough taxonomic prediction
--------------------------
After cleaning the sequences, we can straight away apply the {{kraken_version}} tool to get a rough estimate of the taxonomic composition of the sample. When possible, this classifies each sequence at one of the following levels depending on the accuracy obtained: Domain, Kingdom, Phylum, Class, Order, Family, Genus, or Species. For instance, considering only the Domain level, the breakdown is the following:

{{kraken_domain_table}}

More interestingly, looking at all sequences that were classified at least at the Phylum level we get the following distribution:

{{kraken_phylum_table}}

Next, if one takes only the sequences that were able to be classified at the species level, the 10 most abundant species are:

{{kraken_species_table}}

The full breakdown is available in the file:

    {{kraken_summary_path}}.

-------

Mono Assembly
-------------
Most often, at this point the next step in the pipeline is to combine several samples together in an "aggregate" object and co-assemble them. The result of this operation is available in the report of the corresponding aggregate. But before that, we can try to simply run this sample into the assembler all by itself. For this we use the "{{sample_assembler_version}}" software with a kmer size of {{sample_kmer_size}} and a contig length cutoff of {{sample_contig_cutoff}} base pairs.

This results in the production of {{sample_count_contigs}} contigs, of which the length distribution can be seen in figure \ref{sample_contigs_len_dist}:

{{sample_contigs_len_dist}}

-------

Mono Mapping
------------
Now we will attempt to map every (cleaned) read (excluding singletons) of this sample back to the contigs that were generated in the previous mono-assembly step using the "{{sample_mapper_version}}" software. We will remove the predicted PCR duplicates, leaving us with {{sample_map_filter_count}} reads to map. Exactly {{sample_did_map}} of the reads map back to the contigs generated in the mono-assembly and {{sample_didnt_map}} have no match.

For every of the {{sample_count_contigs}} contigs, we can now compute a mean coverage, as well as the covered fraction. The distribution of these two variables can be seen in figure \ref{sample_mean_coverage} and \ref{samples_percent_covered}:

{{mean_coverage}}

{{percent_covered}}

-------

Co-Mapping
----------
We can also attempt to map every (cleaned) read (excluding singletons) of this sample back to the contigs that were generated in the co-assembly step using the "{{mapper_version}}" software. Again, we will remove the predicted PCR duplicates, leaving us with {{map_filter_count}} reads to map. Exactly {{did_map}} of the reads map back to the contigs generated in the mono-assembly and {{didnt_map}} have no match.

For every of the {{count_contigs}} contigs, we can now compute a mean coverage, as well as the covered fraction. The distribution of these two variables can be seen in figure \ref{mean_coverage} and \ref{percent_covered}:

{{mean_coverage}}

{{percent_covered}}

-------

Annotation
----------
Taking the {{sample_count_contigs}} contigs generated in the previous mono-assembly step, we will attempt annotation using the "{{annotation_version}}" software. This procedure predicts the presence of {{sample_count_proteins}} proteins. When possible, each protein is assigned a function. The most common functions are shown in the table below:

{{sample_functions_table}}

